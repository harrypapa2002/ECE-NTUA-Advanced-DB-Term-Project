{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a19e3ad",
   "metadata": {},
   "source": [
    "# Query 1: Victim Age Group Analysis\n",
    "  \n",
    "## Description: \n",
    "The goal of Query 1 is to rank the victim age groups in incidents involving any form of “aggravated assault” in descending order. The following age groups are defined:  \n",
    "- **Children:** < 18  \n",
    "- **Young Adults:** 18 – 24  \n",
    "- **Adults:** 25 – 64  \n",
    "- **Seniors:** > 64  \n",
    "  \n",
    "This notebook implements the query using both the DataFrame API and the RDD API. Both implementations will be executed using 4 Spark executors, and their execution times will be measured and compared. Finally, the results are saved in CSV files for further analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d86db20",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.executor.instances\": \"4\"\n",
    "    }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90aef977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, count, lower, expr\n",
    "import time\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Query1: Victim Age Group Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Function to measure execution time\n",
    "def measure_execution_time(func, spark):\n",
    "    start_time = time.time()\n",
    "    results = func(spark)\n",
    "    end_time = time.time()\n",
    "    return end_time - start_time, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "571d6ec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Time: 22.04 seconds\n",
      "DataFrame Results:\n",
      "Adults: 121093\n",
      "Young Adults: 33605\n",
      "Children: 15928\n",
      "Seniors: 5985"
     ]
    }
   ],
   "source": [
    "def dataframe_implementation(spark):\n",
    "    # Read the crime datasets for 2010-2019 and 2020-Present, then union them.\n",
    "    crime_data_2010_2019 = spark.read.csv(\n",
    "        \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\",\n",
    "        header=True, inferSchema=True\n",
    "    )\n",
    "    crime_data_2020_present = spark.read.csv(\n",
    "        \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\",\n",
    "        header=True, inferSchema=True\n",
    "    )\n",
    "    crime_data = crime_data_2010_2019.union(crime_data_2020_present)\n",
    "    \n",
    "    # Filter for incidents involving any kind of \"aggravated assault\" (case-insensitive)\n",
    "    aggravated_assault = crime_data.filter(lower(col(\"Crm Cd Desc\")).like(\"%aggravated assault%\"))\n",
    "    \n",
    "    # Categorize victims into age groups\n",
    "    age_grouped = aggravated_assault.withColumn(\n",
    "        \"Age Group\",\n",
    "        when(col(\"Vict Age\") < 18, \"Children\")\n",
    "        .when((col(\"Vict Age\") >= 18) & (col(\"Vict Age\") <= 24), \"Young Adults\")\n",
    "        .when((col(\"Vict Age\") >= 25) & (col(\"Vict Age\") <= 64), \"Adults\")\n",
    "        .otherwise(\"Seniors\")\n",
    "    )\n",
    "    \n",
    "    # Group by \"Age Group\" and count the incidents, ordering by count in descending order\n",
    "    age_group_counts = age_grouped.groupBy(\"Age Group\") \\\n",
    "        .agg(count(\"*\").alias(\"Count\")) \\\n",
    "        .orderBy(col(\"Count\").desc())\n",
    "    \n",
    "    return age_group_counts.collect()\n",
    "\n",
    "# Measure DataFrame implementation time and display results\n",
    "df_time, df_results = measure_execution_time(dataframe_implementation, spark)\n",
    "print(f\"DataFrame Time: {df_time:.2f} seconds\")\n",
    "print(\"DataFrame Results:\")\n",
    "print(\"\\n\".join([f\"{row['Age Group']}: {row['Count']}\" for row in df_results]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9073f1a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD Time: 22.25 seconds\n",
      "RDD Results:\n",
      "Adults: 121093\n",
      "Young Adults: 33605\n",
      "Children: 15928\n",
      "Seniors: 5985"
     ]
    }
   ],
   "source": [
    "def rdd_implementation(spark):\n",
    "    # Read the crime datasets as RDDs for 2010-2019 and 2020-Present.\n",
    "    crime_data_2010_2019_rdd = spark.read.csv(\n",
    "        \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\",\n",
    "        header=True, inferSchema=True\n",
    "    ).rdd\n",
    "    crime_data_2020_present_rdd = spark.read.csv(\n",
    "        \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\",\n",
    "        header=True, inferSchema=True\n",
    "    ).rdd\n",
    "    \n",
    "    # Union the two RDDs\n",
    "    crime_data_rdd = crime_data_2010_2019_rdd.union(crime_data_2020_present_rdd)\n",
    "    \n",
    "    # Filter for rows that contain \"aggravated assault\" (case-insensitive)\n",
    "    aggravated_assault_rdd = crime_data_rdd.filter(\n",
    "        lambda row: \"aggravated assault\" in row[\"Crm Cd Desc\"].lower()\n",
    "    )\n",
    "    \n",
    "\n",
    "    # Map each row to (Age Group, 1) based on \"Vict Age\"\n",
    "    age_group_rdd = aggravated_assault_rdd.map(\n",
    "        lambda row: (\n",
    "            \"Children\" if row[\"Vict Age\"] < 18 else\n",
    "            \"Young Adults\" if 18 <= row[\"Vict Age\"] <= 24 else\n",
    "            \"Adults\" if 25 <= row[\"Vict Age\"] <= 64 else\n",
    "            \"Seniors\", 1\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Reduce by key (Age Group) to count occurrences\n",
    "    age_group_counts_rdd = age_group_rdd.reduceByKey(lambda x, y: x + y)\n",
    "    \n",
    "    # Sort by count in descending order\n",
    "    sorted_age_group_counts = age_group_counts_rdd.sortBy(lambda x: x[1], ascending=False)\n",
    "    \n",
    "    return sorted_age_group_counts.collect()\n",
    "\n",
    "# Measure RDD implementation time and display results\n",
    "rdd_time, rdd_results = measure_execution_time(rdd_implementation, spark)\n",
    "print(f\"RDD Time: {rdd_time:.2f} seconds\")\n",
    "print(\"RDD Results:\")\n",
    "for group, count in rdd_results:\n",
    "    print(f\"{group}: {count}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
