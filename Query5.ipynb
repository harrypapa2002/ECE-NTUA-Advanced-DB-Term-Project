{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d646d060",
   "metadata": {},
   "source": [
    "# Query 5: Nearest Police Station Analysis\n",
    "\n",
    "## Description:\n",
    "Using spatial data, we assign each crime (from the datasets 2010–2019 and 2020–Present) to the nearest police station. Then, for each police division, we compute:\n",
    "- The number of crimes that occurred closest to that division.\n",
    "- The average distance (in km) of those crimes from the police station.\n",
    "\n",
    "The final results are presented sorted in descending order by the crime count.\n",
    "\n",
    "## Resource Configurations Tested:  \n",
    "The query was executed with total resources of 8 cores and 16GB memory in three configurations:\n",
    "- **Configuration 1:** 2 executors × 4 cores / 8GB memory  \n",
    "- **Configuration 2:** 4 executors × 2 cores / 4GB memory  \n",
    "- **Configuration 3:** 8 executors × 1 core / 2GB memory  \n",
    "\n",
    "Finally, the output is saved to an S3 path as a CSV file for further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6a2702",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.executor.instances\": \"2\",\n",
    "        \"spark.executor.memory\": \"8g\",\n",
    "        \"spark.executor.cores\": \"4\"\n",
    "    }\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8551e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.executor.instances\": \"4\",\n",
    "        \"spark.executor.memory\": \"4g\",\n",
    "        \"spark.executor.cores\": \"2\"\n",
    "    }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf36acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.executor.instances\": \"8\",\n",
    "        \"spark.executor.memory\": \"2g\",\n",
    "        \"spark.executor.cores\": \"1\"\n",
    "    }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "198eae35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>2901</td><td>application_1732639283265_2860</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2860/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-119.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_2860_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running query with the following configuration:\n",
      "Executor Instances: 8\n",
      "Executor Memory: 2g\n",
      "Executor Cores: 1"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, min, expr, round as spark_round, count, avg, row_number\n",
    "from sedona.spark import *\n",
    "import time\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Query 5: Nearest Police Station Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "sedona = SedonaContext.create(spark)\n",
    "print(\"Running query with the following configuration:\")\n",
    "conf = spark.sparkContext.getConf()\n",
    "\n",
    "print(\"Executor Instances:\", conf.get(\"spark.executor.instances\"))\n",
    "print(\"Executor Memory:\", conf.get(\"spark.executor.memory\"))\n",
    "print(\"Executor Cores:\", conf.get(\"spark.executor.cores\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "671e26b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------+-----------+\n",
      "| police_division|avg_distance_km|crime_count|\n",
      "+----------------+---------------+-----------+\n",
      "|       HOLLYWOOD|          2.076|     224340|\n",
      "|        VAN NUYS|          2.953|     210134|\n",
      "|       SOUTHWEST|          2.191|     188901|\n",
      "|        WILSHIRE|          2.593|     185996|\n",
      "|     77TH STREET|          1.717|     171827|\n",
      "|         OLYMPIC|          1.724|     170897|\n",
      "| NORTH HOLLYWOOD|          2.643|     167854|\n",
      "|         PACIFIC|           3.85|     161359|\n",
      "|         CENTRAL|          0.992|     153871|\n",
      "|         RAMPART|          1.535|     152736|\n",
      "|       SOUTHEAST|          2.422|     152176|\n",
      "|     WEST VALLEY|          3.036|     138643|\n",
      "|         TOPANGA|          3.297|     138217|\n",
      "|        FOOTHILL|          4.251|     134896|\n",
      "|          HARBOR|          3.703|     126747|\n",
      "|      HOLLENBECK|           2.68|     115837|\n",
      "|WEST LOS ANGELES|          2.792|     115781|\n",
      "|          NEWTON|          1.635|     111110|\n",
      "|       NORTHEAST|          3.624|     108109|\n",
      "|         MISSION|          3.691|     103355|\n",
      "+----------------+---------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Duration: 43.03 seconds"
     ]
    }
   ],
   "source": [
    "# Start the timer to measure execution duration\n",
    "start_time = time.time()\n",
    "\n",
    "# Step 1: Load crime data from the specified CSV files and preprocess\n",
    "crime_df = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ").union(\n",
    "    spark.read.csv(\n",
    "        \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\",\n",
    "        header=True,\n",
    "        inferSchema=True\n",
    "    )\n",
    ").filter((col(\"LAT\") != 0) | (col(\"LON\") != 0)) \\\n",
    " .withColumn(\"crime_point\", expr(\"ST_Point(LON, LAT)\")) \n",
    "\n",
    "# Step 2: Load police station data and create spatial points\n",
    "police_station_df = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/LA_Police_Stations.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ").withColumn(\"station_point\", expr(\"ST_Point(X, Y)\"))  \n",
    "\n",
    "# Step 3: Perform a cross join to compute distances between every crime and every police station\n",
    "crime_station_distance = crime_df.crossJoin(police_station_df).select(\n",
    "    col(\"DR_NO\"),\n",
    "    col(\"crime_point\"),\n",
    "    col(\"DIVISION\").alias(\"police_division\"),\n",
    "    (ST_DistanceSphere(col(\"crime_point\"), col(\"station_point\")) / 1000).alias(\"distance_km\")\n",
    ")\n",
    "\n",
    "# Step 4: Use a window function to find the nearest police station for each crime\n",
    "w = Window.partitionBy(\"DR_NO\").orderBy(\"distance_km\")\n",
    "\n",
    "nearest_station_df = crime_station_distance \\\n",
    "    .withColumn(\"rn\", row_number().over(w)) \\\n",
    "    .filter(col(\"rn\") == 1) \\\n",
    "    .select(\"DR_NO\", \"police_division\", \"distance_km\")\n",
    "\n",
    "# Step 5: Group by police division and compute the average distance and crime count\n",
    "final_result = nearest_station_df.groupBy(\"police_division\").agg(\n",
    "    spark_round(avg(\"distance_km\"), 3).alias(\"avg_distance_km\"),\n",
    "    count(\"*\").alias(\"crime_count\")\n",
    ").orderBy(col(\"crime_count\").desc())\n",
    "\n",
    "final_result.show()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Duration: {round(end_time - start_time, 2)} seconds\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0002ec2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final results saved to: s3://groups-bucket-dblab-905418150721/group21/police_station_analysis/"
     ]
    }
   ],
   "source": [
    "# Define the S3 output path\n",
    "s3_path = \"s3://groups-bucket-dblab-905418150721/group21/police_station_analysis/\"\n",
    "\n",
    "# Save the final results as a CSV\n",
    "final_result.coalesce(1).write.mode(\"overwrite\").csv(s3_path, header=True)\n",
    "print(f\"Final results saved to: {s3_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
